from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

load_dotenv()

model = ChatOpenAI(model="gpt-4", temperature = 1.5, max_completion_tokens=10) # temperature parameter tells how creative the model should be or controls the randomness of the output.
# lower the temperature, the more focused and deterministic the output will be. Higher temperature values will make the output more random.
# code generation lower temperature and for creative work like story telling go for higher temperature.
# max_completion_tokens parameter limits the length of the output generated by the model(tokens are roughly words).
result = model.invoke("What is the capital of India?")
print(result.content)